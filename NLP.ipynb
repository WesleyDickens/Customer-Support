{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9619fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a116765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "#visualization packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP modules we will use for text normalization\n",
    "import re #regex \n",
    "import nltk # the natural language toolkit\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "# feature construction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #use this to create BoW matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae2c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/customer_support_tickets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21eacf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.lda_model\n",
    "\n",
    "\n",
    "#modeling and dimensionality reduction for visuaization\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1b54234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ticket Description'] = df.apply(lambda x: x['Ticket Description'].replace(\"{product_purchased}\", x['Product Purchased']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50e0724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I'm having an issue with the GoPro Hero. Pleas...\n",
       "1       I'm having an issue with the LG Smart TV. Plea...\n",
       "2       I'm facing a problem with my Dell XPS. The Del...\n",
       "3       I'm having an issue with the Microsoft Office....\n",
       "4       I'm having an issue with the Autodesk AutoCAD....\n",
       "                              ...                        \n",
       "8464    My LG OLED is making strange noises and not fu...\n",
       "8465    I'm having an issue with the Bose SoundLink Sp...\n",
       "8466    I'm having an issue with the GoPro Action Came...\n",
       "8467    I'm having an issue with the PlayStation. Plea...\n",
       "8468    There seems to be a hardware problem with my P...\n",
       "Name: Ticket Description, Length: 8469, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Ticket Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85995d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ticket Description'] = df['Ticket Description'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14e6250",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df[['Ticket ID','Product Purchased','Ticket Type','Ticket Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a55b52f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket ID</th>\n",
       "      <th>Product Purchased</th>\n",
       "      <th>Ticket Type</th>\n",
       "      <th>Ticket Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>GoPro Hero</td>\n",
       "      <td>Technical issue</td>\n",
       "      <td>i'm having an issue with the gopro hero. pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>LG Smart TV</td>\n",
       "      <td>Technical issue</td>\n",
       "      <td>i'm having an issue with the lg smart tv. plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dell XPS</td>\n",
       "      <td>Technical issue</td>\n",
       "      <td>i'm facing a problem with my dell xps. the del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Microsoft Office</td>\n",
       "      <td>Billing inquiry</td>\n",
       "      <td>i'm having an issue with the microsoft office....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Autodesk AutoCAD</td>\n",
       "      <td>Billing inquiry</td>\n",
       "      <td>i'm having an issue with the autodesk autocad....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8464</th>\n",
       "      <td>8465</td>\n",
       "      <td>LG OLED</td>\n",
       "      <td>Product inquiry</td>\n",
       "      <td>my lg oled is making strange noises and not fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8465</th>\n",
       "      <td>8466</td>\n",
       "      <td>Bose SoundLink Speaker</td>\n",
       "      <td>Technical issue</td>\n",
       "      <td>i'm having an issue with the bose soundlink sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8466</th>\n",
       "      <td>8467</td>\n",
       "      <td>GoPro Action Camera</td>\n",
       "      <td>Technical issue</td>\n",
       "      <td>i'm having an issue with the gopro action came...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8467</th>\n",
       "      <td>8468</td>\n",
       "      <td>PlayStation</td>\n",
       "      <td>Product inquiry</td>\n",
       "      <td>i'm having an issue with the playstation. plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8468</th>\n",
       "      <td>8469</td>\n",
       "      <td>Philips Hue Lights</td>\n",
       "      <td>Billing inquiry</td>\n",
       "      <td>there seems to be a hardware problem with my p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8469 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ticket ID       Product Purchased      Ticket Type  \\\n",
       "0             1              GoPro Hero  Technical issue   \n",
       "1             2             LG Smart TV  Technical issue   \n",
       "2             3                Dell XPS  Technical issue   \n",
       "3             4        Microsoft Office  Billing inquiry   \n",
       "4             5        Autodesk AutoCAD  Billing inquiry   \n",
       "...         ...                     ...              ...   \n",
       "8464       8465                 LG OLED  Product inquiry   \n",
       "8465       8466  Bose SoundLink Speaker  Technical issue   \n",
       "8466       8467     GoPro Action Camera  Technical issue   \n",
       "8467       8468             PlayStation  Product inquiry   \n",
       "8468       8469      Philips Hue Lights  Billing inquiry   \n",
       "\n",
       "                                     Ticket Description  \n",
       "0     i'm having an issue with the gopro hero. pleas...  \n",
       "1     i'm having an issue with the lg smart tv. plea...  \n",
       "2     i'm facing a problem with my dell xps. the del...  \n",
       "3     i'm having an issue with the microsoft office....  \n",
       "4     i'm having an issue with the autodesk autocad....  \n",
       "...                                                 ...  \n",
       "8464  my lg oled is making strange noises and not fu...  \n",
       "8465  i'm having an issue with the bose soundlink sp...  \n",
       "8466  i'm having an issue with the gopro action came...  \n",
       "8467  i'm having an issue with the playstation. plea...  \n",
       "8468  there seems to be a hardware problem with my p...  \n",
       "\n",
       "[8469 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c4907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "def process_ticket(ticket_text, min_length):\n",
    "    \n",
    "    # get common stop words that we'll remove during tokenization/text normalization\n",
    "    stop_words = list(get_stop_words('en'))         #About 900 stopwords\n",
    "    nltk_words = list(stopwords.words('english')) #About 150 stopwords\n",
    "    stop_words.extend(nltk_words)\n",
    "\n",
    "    #initialize lemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "   \n",
    "\n",
    "    # lower case everything\n",
    "    ticket_lower = ticket_text.lower()\n",
    "\n",
    "    ticket_lower = re.sub(r\"@[a-z0-9_]+|#[a-z0-9_]+|http\\S+\", \"\", ticket_lower).strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "\n",
    "    # remove stop words and punctuations \n",
    "    \n",
    "    ticket_norm = [x for x in word_tokenize(ticket_lower) if ((x.isalpha()) & (x not in stop_words)) ]\n",
    "\n",
    "    #  POS detection on the result will be important in telling Wordnet's lemmatizer how to lemmatize\n",
    "    \n",
    "    # creates list of tuples with tokens and POS tags in wordnet format\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(ticket_norm))) \n",
    "\n",
    "    # now we are going to have a cutoff here. any tokenized cocument with length < min length will be removed from corpus\n",
    "    if len(wordnet_tagged) <= min_length:\n",
    "        return ''\n",
    "    else:\n",
    "         # rejoins lemmatized sentence \n",
    "        ticket_norm = \" \".join([wnl.lemmatize(x[0], x[1]) for x in wordnet_tagged if x[1] is not None])\n",
    "        return ticket_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "600d981a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/3hvhfs550_j07qx5c00fkgnc0000gn/T/ipykernel_76789/1391231502.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Ticket Description Cleaned'] = cleaned_df['Ticket Description'].apply(process_ticket, args = [10])\n"
     ]
    }
   ],
   "source": [
    "cleaned_df['Ticket Description Cleaned'] = cleaned_df['Ticket Description'].apply(process_ticket, args = [10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f58e8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# canon_df = cleaned_df[cleaned_df['Product Purchased']=='Canon EOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f9e2ca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for ticket_type in canon_df['Ticket Type'].unique():\n",
    "#     corpus = canon_df[canon_df['Ticket Type']==ticket_type]['Ticket Description Cleaned']\n",
    "#     print(ticket_type)\n",
    "#     print('-------')\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     X_train = vectorizer.fit_transform(corpus)\n",
    "#     X_train\n",
    "\n",
    "#     topic_model = NMF(n_components = 5)\n",
    "#     topic_model.fit(X_train)\n",
    "\n",
    "#     # to get H\n",
    "#     H = topic_model.transform(X_train) # transform document into topic vector representation\n",
    "\n",
    "#     # to get W \n",
    "#     W = topic_model.components_ # word component weights for each topic\n",
    "\n",
    "#     for index,topic in enumerate(W):\n",
    "#         print(f'THE TOP 10 WORDS FOR TOPIC #{index}')\n",
    "#         print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-25:]])\n",
    "#         print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45af3313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/3hvhfs550_j07qx5c00fkgnc0000gn/T/ipykernel_76789/488352925.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Assigned Topic'] = None\n",
      "/var/folders/_x/3hvhfs550_j07qx5c00fkgnc0000gn/T/ipykernel_76789/488352925.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Topic Words'] = None\n",
      "/var/folders/_x/3hvhfs550_j07qx5c00fkgnc0000gn/T/ipykernel_76789/488352925.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_df['Topic Words'] = cleaned_df['Assigned Topic'].map(topic_words_dict)\n"
     ]
    }
   ],
   "source": [
    "# Check if 'Assigned Topic' and 'Topic Words' columns exist, if not, create them\n",
    "if 'Assigned Topic' not in cleaned_df.columns:\n",
    "    cleaned_df['Assigned Topic'] = None\n",
    "if 'Topic Words' not in cleaned_df.columns:\n",
    "    cleaned_df['Topic Words'] = None\n",
    "\n",
    "# Initialize a dictionary to store top words for each topic of each product and ticket type\n",
    "topic_words_dict = {}\n",
    "\n",
    "# Iterate over each product\n",
    "for product in cleaned_df['Product Purchased'].unique():\n",
    "    # Filter for tickets related to the current product\n",
    "    product_df = cleaned_df[cleaned_df['Product Purchased'] == product]\n",
    "    \n",
    "    for ticket_type in product_df['Ticket Type'].unique():\n",
    "        # Selecting the subset of the DataFrame for the current ticket type\n",
    "        subset_df = product_df[product_df['Ticket Type'] == ticket_type]\n",
    "        corpus = subset_df['Ticket Description']\n",
    "        ticket_ids = subset_df.index  # Using DataFrame index as a proxy for ticket ID if 'Ticket ID' column does not exist\n",
    "\n",
    "        # Initialize TF-IDF Vectorizer and NMF Model\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train = vectorizer.fit_transform(corpus)\n",
    "        topic_model = NMF(n_components=5, random_state=42)  # Added random_state for reproducibility\n",
    "        topic_model.fit(X_train)\n",
    "\n",
    "        # Transform document into topic vector representation\n",
    "        H = topic_model.transform(X_train)  # Document-topic matrix\n",
    "        W = topic_model.components_  # Topic-term matrix\n",
    "\n",
    "        # For each topic, store the top words\n",
    "        for topic_idx, topic in enumerate(W):\n",
    "            top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-20:]]  # Get top 10 words\n",
    "            topic_key = f\"{product}_{ticket_type}_Topic{topic_idx}\"\n",
    "            topic_words_dict[topic_key] = ', '.join(top_words)  # Store as a comma-separated string\n",
    "\n",
    "        # Assign tickets to topics based on the highest topic weight\n",
    "        for ticket_index, topic_vector in zip(ticket_ids, H):\n",
    "            # Find the topic with the highest weight for this ticket\n",
    "            assigned_topic = topic_vector.argmax()\n",
    "            topic_key = f\"{product}_{ticket_type}_Topic{assigned_topic}\"\n",
    "            # Update the 'Assigned Topic' in cleaned_df directly\n",
    "            cleaned_df.at[ticket_index, 'Assigned Topic'] = topic_key\n",
    "\n",
    "# Map the 'Assigned Topic' to its corresponding top words\n",
    "cleaned_df['Topic Words'] = cleaned_df['Assigned Topic'].map(topic_words_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47246459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19      please, product, ensure, safe, security, conce...\n",
       "36      an, with, eos, canon, could, you, me, option, ...\n",
       "85      factory, performed, password, is, can, canon, ...\n",
       "144     working, turning, now, yesterday, until, not, ...\n",
       "199     could, update, that, started, and, please, can...\n",
       "                              ...                        \n",
       "8222    using, my, on, settings, and, configurations, ...\n",
       "8229    peripherals, adapters, cables, ve, different, ...\n",
       "8324    please, product, ensure, safe, security, conce...\n",
       "8341    would, other, are, the, devices, fine, doesn, ...\n",
       "8349    assist, right, canon, eos, steps, you, in, me,...\n",
       "Name: Topic Words, Length: 240, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df[cleaned_df['Product Purchased']=='Canon EOS']['Topic Words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb188ec5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Echo\n",
      "Technical issue\n",
      "you, kai, thank, issue, bug, amazon, echo, of, related, to, recently, firmware, happening, afterward, updated, could, be, started, update, the\n",
      "\n",
      "i'm having an issue with the amazon echo. please assist. i'll fix it as quickly as i can! thank you.\"\n",
      "\n",
      "it's not just the text that changes these numbers that makes the results appear to be wrong i've recently updated the firmware of my amazon echo, and the issue started happening afterward. could it be related to the update?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(0, cleaned_df.shape[0])\n",
    "\n",
    "# cleaned_df.iloc[[index]]\n",
    "\n",
    "product = cleaned_df.iloc[index]['Product Purchased']\n",
    "issue_type = cleaned_df.iloc[index]['Ticket Type']\n",
    "topic_words = cleaned_df.iloc[index]['Topic Words']\n",
    "description = cleaned_df.iloc[index]['Ticket Description']\n",
    "\n",
    "print(product)\n",
    "print(issue_type)\n",
    "print(topic_words)\n",
    "print()\n",
    "print(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00b3a543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry to hear that you're experiencing technical issues with your Amazon Echo following a recent firmware update. To resolve this issue, I recommend performing a factory reset on your Amazon Echo. This can help clear out any bugs or glitches that may have occurred due to the update. Here's how you can do it:\n",
      "\n",
      "1. Press and hold the \"Action\" button on your Amazon Echo for about 25 seconds until the light ring turns orange. \n",
      "2. Once the light ring turns blue, Alexa will greet you. Follow the instructions to complete the setup process again.\n",
      "\n",
      "Performing a factory reset should help resolve any issues you're facing with your Amazon Echo post the firmware update.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key = api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a customer support bot. Your job is to give helpful advice when a customer writes in with a given issue. We have used a machine learning model to assign a topic to their specific request based on the top 20 words associated with that topic. The response should always contain a resolution, there will be no opportunity for followup from the user.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Product is {product}, Issue type is {issue_type}, Topic words are {topic_words}\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc6cb5",
   "metadata": {},
   "source": [
    "# GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83b61805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = vectorizer.fit_transform(cleaned_df['Ticket Description'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6997d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path):\n",
    "    embeddings_dict = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "# Example path, adjust as needed\n",
    "glove_path = 'Data/glove.6B.100d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1699a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_glove_vector(text, glove_embeddings):\n",
    "    word_vectors = []\n",
    "    # Check if text is None and treat it as an empty string\n",
    "    text = text if text is not None else \"\"\n",
    "    for word in text.split():\n",
    "        if word in glove_embeddings:\n",
    "            word_vectors.append(glove_embeddings[word])\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        # Return a zero vector if the document is empty\n",
    "        return np.zeros(100)  # 100-dimensional GloVe vectors\n",
    "\n",
    "# Apply this function to each document\n",
    "glove_vectors = np.array([document_to_glove_vector(doc, glove_embeddings) for doc in cleaned_df['Ticket Description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c71e5a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # For TF-IDF vectors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# tsne_model_tfidf = TSNE(n_components=2, random_state=42)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# tsne_tfidf = tsne_model_tfidf.fit_transform(tfidf_vectors.toarray())  # Convert sparse matrix to dense\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# For GloVe vectors\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tsne_model_glove \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m tsne_glove \u001b[38;5;241m=\u001b[39m tsne_model_glove\u001b[38;5;241m.\u001b[39mfit_transform(glove_vectors)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1111\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \n\u001b[1;32m   1092\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m-> 1111\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:952\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    946\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[t-SNE] Indexed \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m samples in \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124ms...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    947\u001b[0m             n_samples, duration\n\u001b[1;32m    948\u001b[0m         )\n\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    951\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m--> 952\u001b[0m distances_nn \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mkneighbors_graph(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    953\u001b[0m duration \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_base.py:986\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors_graph\u001b[0;34m(self, X, n_neighbors, mode)\u001b[0m\n\u001b[1;32m    983\u001b[0m     A_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(n_queries \u001b[38;5;241m*\u001b[39m n_neighbors)\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 986\u001b[0m     A_data, A_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkneighbors(X, n_neighbors, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    987\u001b[0m     A_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mravel(A_data)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_base.py:822\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    815\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[1;32m    818\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[1;32m    819\u001b[0m     )\n\u001b[1;32m    820\u001b[0m )\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[0;32m--> 822\u001b[0m     results \u001b[38;5;241m=\u001b[39m ArgKmin\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    823\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    824\u001b[0m         Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[1;32m    825\u001b[0m         k\u001b[38;5;241m=\u001b[39mn_neighbors,\n\u001b[1;32m    826\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_,\n\u001b[1;32m    827\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_,\n\u001b[1;32m    828\u001b[0m         strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    829\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    830\u001b[0m     )\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[1;32m    834\u001b[0m ):\n\u001b[1;32m    835\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[1;32m    836\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[1;32m    837\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:270\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[0;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin64\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    259\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    260\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    271\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    272\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m    273\u001b[0m         k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    274\u001b[0m         metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m    275\u001b[0m         chunk_size\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[1;32m    276\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39mmetric_kwargs,\n\u001b[1;32m    277\u001b[0m         strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[1;32m    278\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly float64 or float32 datasets pairs are supported at this time, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot: X.dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and Y.dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m )\n",
      "File \u001b[0;32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:575\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/fixes.py:72\u001b[0m, in \u001b[0;36mthreadpool_limits\u001b[0;34m(limits, user_api)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m threadpoolctl\u001b[38;5;241m.\u001b[39mthreadpool_limits(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:171\u001b[0m, in \u001b[0;36mthreadpool_limits.__init__\u001b[0;34m(self, limits, user_api)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(limits, user_api)\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_threadpool_limits()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:268\u001b[0m, in \u001b[0;36mthreadpool_limits._set_threadpool_limits\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m modules \u001b[38;5;241m=\u001b[39m _ThreadpoolInfo(prefixes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes,\n\u001b[1;32m    269\u001b[0m                           user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# self._limits is a dict {key: num_threads} where key is either\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# a prefix or a user_api. If a module matches both, the limit\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# corresponding to the prefix is chosed.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:340\u001b[0m, in \u001b[0;36m_ThreadpoolInfo.__init__\u001b[0;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m user_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m user_api\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_modules()\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:371\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._load_modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loop through loaded libraries and store supported ones\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dyld()\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_enum_process_module_ex()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:428\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._find_modules_with_dyld\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m filepath \u001b[38;5;241m=\u001b[39m filepath\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# Store the module if it is supported and selected\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_module_from_path(filepath)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:515\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._make_module_from_path\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefixes \u001b[38;5;129;01mor\u001b[39;00m user_api \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api:\n\u001b[1;32m    514\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[module_class]\n\u001b[0;32m--> 515\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class(filepath, prefix, user_api, internal_api)\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mappend(module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:606\u001b[0m, in \u001b[0;36m_Module.__init__\u001b[0;34m(self, filepath, prefix, user_api, internal_api)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_api \u001b[38;5;241m=\u001b[39m internal_api\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(filepath, mode\u001b[38;5;241m=\u001b[39m_RTLD_NOLOAD)\n\u001b[0;32m--> 606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_version()\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_threads()\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_extra_info()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:646\u001b[0m, in \u001b[0;36m_OpenBLASModule.get_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m get_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenblas_get_config\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    644\u001b[0m                      \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    645\u001b[0m get_config\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[0;32m--> 646\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenBLAS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# # For TF-IDF vectors\n",
    "# tsne_model_tfidf = TSNE(n_components=2, random_state=42)\n",
    "# tsne_tfidf = tsne_model_tfidf.fit_transform(tfidf_vectors.toarray())  # Convert sparse matrix to dense\n",
    "\n",
    "# For GloVe vectors\n",
    "tsne_model_glove = TSNE(n_components=2, random_state=42)\n",
    "tsne_glove = tsne_model_glove.fit_transform(glove_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7fc4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c0e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
